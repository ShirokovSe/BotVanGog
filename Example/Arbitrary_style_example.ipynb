{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wRLYU5CfpqXz"},"outputs":[],"source":["import torchvision\n","import torch\n","import torch.nn as nn\n","from torchsummary import summary\n","from torchvision import transforms\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision.datasets import ImageFolder\n","from PIL import Image, ImageFile\n","from tqdm.autonotebook import tqdm\n","import glob\n","import os\n","import torch.backends.cudnn as cudnn\n","from matplotlib import pyplot as plt\n","import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18990,"status":"ok","timestamp":1643278647785,"user":{"displayName":"Serge Shirokov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09945367844859667876"},"user_tz":-180},"id":"_Udbw8oqrlm1","outputId":"0c6be6ea-5b58-4a4c-c2a0-d237b186bcdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["Изменяю параметры загрузки изображений для избежания проблем при формировании загрузчика данных"],"metadata":{"id":"q_plQjT13-v3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYPdC-cCGgxl"},"outputs":[],"source":["cudnn.benchmark = True\n","Image.MAX_IMAGE_PIXELS = None \n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"markdown","source":["Инициализация основных переменных"],"metadata":{"id":"Hm-hp1QPHVCb"}},{"cell_type":"code","source":["EPOHS = 500\n","BATCH_SIZE = 4\n","lr = 1e-4\n","IMAGE_SIZE = 512\n","IMAGE_SIZE_CROP = 256\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"osMqwmJeHUeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Подготовка среды для загрузки датасета с Kaggle"],"metadata":{"id":"pMMI7fH24lem"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlxfkfkNrbzL"},"outputs":[],"source":["! mkdir ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiKmdrXmres4"},"outputs":[],"source":["!cp /content/gdrive/MyDrive/Project/Style\\ transfer/kaggle.json ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tD1yWPmsr4Lv"},"outputs":[],"source":["!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"markdown","source":["датасет был взят с соревнования: https://www.kaggle.com/c/painter-by-numbers"],"metadata":{"id":"4oYuYrO24wXs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzDNv67wr4OW"},"outputs":[],"source":["!kaggle competitions download -c painter-by-numbers -f train.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8n5YcNyQwxzl"},"outputs":[],"source":["!unzip /content/train.zip -d /content/style_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5I3Lfi3AB3hg"},"outputs":[],"source":["!rm /content/train.zip"]},{"cell_type":"markdown","source":["Проверяем размер датасета"],"metadata":{"id":"agqtoClP5Knq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1643103833460,"user":{"displayName":"Serge Shirokov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09945367844859667876"},"user_tz":-180},"id":"rQmK_QjcvXHa","outputId":"485bc0d6-1883-4b1c-f02f-0fbefe928d41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["79433"]},"metadata":{},"execution_count":13}],"source":["len(os.listdir('/content/style_dataset/train'))"]},{"cell_type":"markdown","source":["второй датасета для обучения COCO-2017"],"metadata":{"id":"6Z81nDJZ5KFe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JudxWXcJr4Sk"},"outputs":[],"source":["!pip install fiftyone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdCUHJ_5uQq-"},"outputs":[],"source":["!pip uninstall  opencv-python-headless==4.5.5.62"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXvnyux_uRfT"},"outputs":[],"source":["!pip install opencv-python-headless==4.1.2.30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyA5-2InuQti"},"outputs":[],"source":["import fiftyone as fo\n","import fiftyone.zoo as foz\n","\n","dataset = foz.load_zoo_dataset(\n","    \"coco-2017\",\n","    split = 'train'\n",")"]},{"cell_type":"markdown","source":["Функция для нахождения среднеквадратического отклонения"],"metadata":{"id":"vyEpZ0XvG7dL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU0Mv3F4p5M2"},"outputs":[],"source":["def find_mean_std(input, eps = 1e-5):\n","  batch_size, channels, height, weight = input.size()\n","  input_std = torch.sqrt(input.view(batch_size, channels,-1).var(dim=2) + eps).view(batch_size, channels,1,1)\n","  input_mean = torch.mean(input.view(batch_size, channels,-1), dim = 2).view(batch_size, channels,1,1)\n","  \n","  return input_mean, input_std"]},{"cell_type":"markdown","source":["Функция для расчет адаптивной поканальной нормализации"],"metadata":{"id":"KZ3byoIZH5vx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiO9HrplcMTA"},"outputs":[],"source":["def AdaIN(content, style):\n","  content_mean, content_std = find_mean_std(content)\n","  style_mean, style_std = find_mean_std(style)\n","\n","  return style_std * ((content - content_mean) / content_std ) + style_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifs-at30qNx_"},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.model = nn.Sequential(\n","\n","        nn.Conv2d(512,256, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n","\n","        nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Conv2d(256,128, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace=True),\n","        nn.Upsample(scale_factor = 2,mode = 'nearest'),\n"," \n","        nn.Conv2d(128,128, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Conv2d(128,64,kernel_size=3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Upsample(scale_factor = 2, mode='nearest'),\n","        nn.Conv2d(64,64, kernel_size = 3, stride = 1, padding = 1, padding_mode='reflect'),\n","        nn.ReLU(inplace = True),\n","        nn.Conv2d(64,3, kernel_size = 3, padding = 1, padding_mode='reflect'),  \n","    )\n","\n","  def forward(self,x):\n","    return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6a9pNDFUxtn"},"outputs":[],"source":["decoder = Decoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhjpYQ03LbYs"},"outputs":[],"source":["class Net(nn.Module):\n","  def __init__(self,decoder):\n","    super().__init__()\n","    self.encoder = torchvision.models.vgg19(pretrained=True).features[:21]\n","    self.decoder = decoder\n","    self.mse_loss = nn.MSELoss()\n","\n","    #заменяем тип паддинга в энкодере\n","    for module in self.encoder.modules():\n","        classname = module.__class__.__name__\n","        if 'Conv' in classname:\n","            module.padding_mode = 'reflect'\n","    #энкодер не обучается\n","    for parameter in self.encoder.parameters():\n","      parameter.requires_grad_(False)\n","\n","  def decode(self,x):\n","    return self.decoder(x)\n","\n","  def encode(self,x):\n","    return self.encoder(x)\n","\n","  def encode_per_layer(self,x):\n","\n","    features = []\n","\n","    for layer_num,layer in enumerate(self.encoder):\n","      x = layer(x)\n","\n","      if layer_num in [1,6,11,21]:\n","        features.append(x)\n","\n","    return features\n","\n","  def content_loss(self,x, content):\n","    return self.mse_loss(x, content)\n","\n","\n","  def style_loss(self, x, style):\n","    mean_st, std_st = find_mean_std(style)\n","    mean_inp, std_inp = find_mean_std(x)\n","\n","    return self.mse_loss(mean_inp, mean_st) + self.mse_loss(std_inp, std_st)\n","\n","  def forward(self, content, style, alpha = 1.0):\n","    style_f = self.encode_per_layer(style)\n","    content_f = self.encode_per_layer(content)\n","\n","    normal = AdaIN(content_f[-1],style_f[-1])\n","\n","    generated = self.decoder((1 - alpha) * content_f[-1] + alpha * normal)\n","    generated = self.encode_per_layer(generated[-1])\n","\n","    loss_cont = self.content_loss(generated[-1], normal)\n","    loss_style = self.style_loss(generated[0], style_f[0])\n","    \n","    for layer in range(1,4):\n","        loss_style += self.style_loss(generated[layer], style_f[layer])\n","\n","    return loss_cont, loss_style"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPINNRVOxjFt"},"outputs":[],"source":["net = Net(decoder).to(DEVICE)\n","optimizer = torch.optim.Adam(net.decoder.parameters(), lr =2e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niwa_dTp15U1"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize(IMAGE_SIZE),\n","    transforms.RandomCrop((IMAGE_SIZE_CROP),(IMAGE_SIZE_CROP)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = (0.5,0.5,0.5),std = (0.5,0.5,0.5]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9LwMZPmOhLG"},"outputs":[],"source":["DIR_content = '/root/fiftyone/coco-2017/train'\n","DIR_style = '/content/style_dataset'"]},{"cell_type":"code","source":["#проверка файлов\n","for image in style_dataset:\n","    for file,label in tqdm(image):\n","        try:\n","            im = ImageFile.Image.open(file)\n","            im2 = im.convert('RGB')\n","        except DecompressionBombError:\n","            print(\"Cannot load : {}\".format(fn))"],"metadata":{"id":"3oZ1Geg_J0RP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgitwCpRA2C4"},"outputs":[],"source":["#часть файлов в датасете оказалась поврежденная, поэтому их сразу пришлось исключить из датасета\n","corrupted = ['3917','41945','79499','91033','92899','95347','101947']\n","for file in corrupted:\n","  os.system(f'rm /content/style_dataset/train/{file}.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVvTHd461zMO"},"outputs":[],"source":["content_dataset = ImageFolder(DIR_content, transform=transform)\n","content_loader = DataLoader(content_dataset, batch_size=BATCH_SIZE,  pin_memory = True, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8t3h0l6O2r7j"},"outputs":[],"source":["style_dataset = ImageFolder(DIR_style, transform = transform)\n","style_loader = DataLoader(style_dataset, batch_size = BATCH_SIZE, pin_memory = True, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEp-Kc8F1CQS"},"outputs":[],"source":["def test_transform(size, crop):\n","    transform_list = []\n","    if size != 0:\n","        transform_list.append(transforms.Resize(size))\n","    if crop:\n","        transform_list.append(transforms.CenterCrop(size))\n","    transform_list.append(transforms.ToTensor())\n","    transform = transforms.Compose(transform_list)\n","    return transform\n","    \n","def denorm(x):\n","  stats = (0.5,0.5,0.5),(0.5,0.5,0.5)\n","  return x*stats[0][0] + stats[0][1]\n","\n","transform= test_transform(2048, True)\n","\n","#картинки для отслеживания качества переноса стиля\n","content_image = Image.open('./content.png')\n","style_image = Image.open('./style.jpg')\n","content = transform(cont).unsqueeze(0).cuda()\n","style = transform(style).unsqueeze(0).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLvPbSJ1dvgc"},"outputs":[],"source":["iteration = 0\n","for epoch in tqdm(range(4)):\n","    for (batch_st, _) , (batch_cont, _) in tqdm(zip(style_loader, content_loader)):\n","      net.decoder.train()\n","      iteration +=1\n","\n","      batch_st = batch_st.to(DEVICE)\n","      batch_cont = batch_cont.to(DEVICE)\n","      loss_cont, loss_st = net(batch_cont, batch_st)\n","      loss = loss_cont + 10 * loss_st\n","\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      if iteration % 500 == 0 :\n","        print('Iteration: {:.4f} Loss style: {:.4f}, Loss content: {:.4f}, Full loss: {:.4f}'.format(\n","            iteration, loss_st.item(), loss_cont.item(), loss.item()))\n","        \n","        content_f = net.encoder(content)\n","        style_f = net.encoder(style)\n","        normalized = AdaIN(content_f,  style_f)\n","\n","        net.decoder.eval()\n","        with torch.no_grad():\n","          out = net.decoder(normalized)\n","        \n","        torch.save({\n","          'decoder':net.decoder.state_dict(),\n","          'optim':optimizer.state_dict()}, './check.tar')\n","        plt.subplot(1,3,1)\n","        plt.imshow(content_image)\n","        plt.subplot(1,3,2)\n","        plt.imshow(style_image)\n","        plt.subplot(1,3,3)\n","        plt.imshow((denorm(out).squeeze(0).cpu().numpy().transpose(1,2,0)* 255).astype(np.uint8))\n","        plt.show();"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Arbitrary_style_example.ipynb","provenance":[],"authorship_tag":"ABX9TyM1xbQlT14+d7urnYs7CxNE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}